## 缓存失效

### 问题描述

大量的key同时过期，cache访问miss，穿透到DB，DB压力大增，慢查率增大

1. 预设设置固定 expire time
2. 主动/被动 批量DB加载，写入缓存时，设置预设过期时间
3. 达到过期时间，批量一起过期
4. 批量请求一起穿透到DB

### 原因

跟我们日常写缓存的过期时间息息相关

### 业务场景

- 同一批次火车票，发布时，一次性加载到缓存
- 后台系统，计算出热门微博，批量加载到缓存
- 缓存预热，一次性加载最新N小时数据

### 解决方案

过期时间策略：base+random，基础过期时间加上随机过期时间

## 缓存穿透

### 问题描述

查询一个不存在的key，每次访问都穿刺到DB，对DB造成压力。

1. 系统访问策略，cache miss后会查DB
2. 正常的key，会查询到value并回写
3. 正常的key，后续查询会直接命中缓存并返回
4. 不存在的key，没有查到value，直接返回null
5. 不存在的key，后续所有查询都会cache miss，全部访问DB

### 原因

系统设计，更多考虑的是正常路径，对特殊访问路径考虑相对欠缺

### 业务场景

- 通过不存在的ID，访问不存在的用户
- 通过不存在的车次ID，访问不存在的车票信息

### 解决方案

- 不存在的key，在cache中设置一个默认值
- 构建`BloomFilter`过滤器，过滤器非法key访问

### BloomFilter

**原理**

bit 数组来表示一个集合，多次hashcheck，全部为 1 表示存在，否则表示不存在

**算法**

内存bit数组，初期值全为零；加入元素，采用k个相互独立的hash函数计算，将元素hash映射的k个位置设置为1

**优势**

- 全内存操作，性能高
- 空间效率高，误判率极低

## 缓存雪崩

### 问题描述

部分缓存节点不可用，导致整个缓存体系、服务系统不可用的情况，网上解释场景同缓存失效。

**缓存不支持rehash导致的系统雪崩不可用**

1. 缓存不支持rehash
2. 较多缓存节点不可用
3. 大量Cache访问失败，进一步访问DB
4. DB可承载的访问量只有缓存 1-2% 以下，DB过载，服务异常

**缓存支持rehash导致的系统雪崩不可用**

1. 缓存支持rehash
2. 流量洪峰到达，大流量key集中在其中1-2个节点
3. 大流量key所在缓存节点过载crash，异常节点下线
4. 请求rehash，进一步导致其他缓存节点也过载crash
5. 恶性循环，最终整个缓存体系雪崩不可用

### 原因

- 缓存不rehash，雪崩跟较多缓存节点不可用有关，大量请求穿透，DB过载不可用，进而导致整个服务异常
- 缓存支持rehash，雪崩跟流量洪峰有关，部分缓存节点过载crash，过载扩散，引发整个缓存异常不可用

### 业务场景

- 突发洪水流量，部分cache过载，进而整个cache池过载异常
- 计架断电，cache节点宕机，大量请求打到DB，服务异常

### 解决方案

- 增加DB读写开关，慢查询超过阈值，关闭开关
- 多副本cache架构，任何cache池miss后，读其他cache副本
- 实时监控，及时发现并恢复，增加自动故障转移策略

## 数据不一致

### 问题描述

**cache中数据与DB数据不一致**

1. 更新DB后，更新cache失败，cache存的是老数据
2. 采用rehash自动漂移策略，在节点多次上下线之后，也会产生脏数据

**多个cache副本，不同cache副本中的数据不一致**

多缓存副本，某个缓存副本写异常，更新失败导致副本间数据不一致

### 业务场景

- cache机带宽打满/网络波动，更新cache失败
- 缓存rehash，节点异常，多次上下线

### 解决方案

- cache更新失败后，重试或者延迟删除
- 调短过期时间，DB重新加载，最终一致性
- 拒绝rehash漂移，采用缓存分层策略

## 数据并发竞争

### 问题描述

高并发场景，缓存miss，并发到DB查相同key

1. 大量并发请求相同key，key在cache中不存在
2. 进程加载没有协调，并发查询DB，DB压力大增

### 业务场景

- 某车次缓存信息过期，仍有大量用户同时查询该车次信息
- 某条微博缓存数据淘汰，仍有大量转发、评论、访问该条微博

### 解决方案

- 缓存miss后，加全局锁，唯一进程查DB，并回写
- 其他进程查询，若缓存miss，先查是否被锁定，发现有锁就等待
- 等加锁进程回写完毕后，从缓存加载
- 或者直接缓存数据保持多个备份，减少miss的概率

## Hot Key

### 问题描述

突发热门事件，特定key所在的cache节点服务过载、卡顿

1. 突发热门事件发生，超大量请求计中访问热门事件对应的key（10W-100W+用户一起访问）
2. 流量集中打在一个cache节点机器，达到物理网卡、带宽、CPU极限，从而导致服务异常

### 业务场景

- 明星结婚/出轨等特殊事件
- 奥运、春节等重大活动、节日
- 秒杀、双11、618等线上促销活动

### 解决方案

- hotkey分散存放
- 相同的key分散：多次缓存，多slave
- SLA监控，支持快速扩容
- 业务端本地缓存记录极热key

### 查找Hot key

- 预先评估重要节假日相关key、秒杀活动的商品、集中推送的内容
- 批处理任务离线计算，找出最近历史最高频关键词（Hadoop）
- 流任务实时分析，及时发现新发布的热门数据（Spark）

## Big key

### 问题描述

部分Key的Value过大，读写、加载易超时

1. Big key存在mc，对应的slab较少，导致频繁被剔除，DB反复加载
2. Key的Value过大，如果此类型key被大量访问，缓存组件的带宽、网卡被打满
3. Key存储的字段过多，每个字段变成都需要对缓存数据进行变更，读写互相影响
4. Key如果被缓存系统淘汰，DB加载需要查询大量数据，加载时间长

### 业务场景

- 用户最新1W个粉丝的存储
- 用户个人信息，包括基本资料，关系图谱计数，发Feed统计等
- 用户微博一般140字节以内，有些用户发1K左右的长微博

### 解决方案

**memcache**

- 设置缓存阈值，超过阈值即启动压缩
- 启动后预写足够数量大的大key，预分配对应slab

**Redis**

- Client 序列化，restore一次性写入

**其他**

- Big key分拆为多个小key
- 尽量避免Big key过期或剔除
